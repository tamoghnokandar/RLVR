# -*- coding: utf-8 -*-
"""Without external rewards GRPO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/151BB_o-h3u8pE0nxVsMYbukcZDjqOad3
"""
# scp -i private_key.pem -P 18600 deepspeed_zero3.yaml root@78.130.201.2:/root
# scp -i private_key.pem -P 18600 without_external_rewards_grpo.py root@78.130.201.2:/root
# python -m venv grpo
# source grpo/bin/activate
# pip install uv
# uv pip install transformers trl vllm bitsandbytes deepspeed wandb peft datasets 
# pip install -U datasets
# pip install "trl[vllm]"
# CUDA_VISIBLE_DEVICES=0 trl vllm-serve --model Qwen/Qwen3-1.7B --tensor_parallel_size 1
# CUDA_VISIBLE_DEVICES=1,2,3,4 accelerate launch --num-processes 4 --config-file deepspeed_zero3.yaml without_external_rewards_grpo.py
# CUDA_VISIBLE_DEVICES=1,2,3,4 nohup accelerate launch --num-processes 4 --config-file deepspeed_zero3.yaml without_external_rewards_grpo.py > output_grpo.log 2>&1 &
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from trl import GRPOConfig, GRPOTrainer
import wandb
import os
# Login using e.g. `huggingface-cli login` to access this dataset

import re
import ast
from collections import Counter
   
global PRINTED_TIMES
PRINTED_TIMES = 0
global PRINT_EVERY_STEPS
PRINT_EVERY_STEPS = 5



def canonicalize_groups(groups):
    """Sorts words within each group and sorts the groups themselves
    to allow for direct comparison, regardless of order."""
    if not isinstance(groups, list):
        return None
    try:
        # Sort words inside each group
        sorted_word_groups = [sorted(group) for group in groups]
        # Sort the groups themselves (represented as tuples to be hashable)
        return sorted(tuple(group) for group in sorted_word_groups)
    except (TypeError, AttributeError):
        # Handle cases where the prediction is not a list of lists
        return None
def structured_reward(generated_lists, ground_truth_lists):
  """
  Calculates a structured reward based on element, list, and goal-level achievements.
  """
  total_reward = 0
  num_perfect_lists = 0

  for gen_list, gt_list in zip(generated_lists, ground_truth_lists):
    gen_set = set(gen_list)
    gt_set = set(gt_list)


    # Check for a perfect list match
    if gen_set == gt_set:
      total_reward += 2  # List-level bonus
      num_perfect_lists += 1
  
    # Element-level rewards and penalties
    correct_elements = len(gen_set.intersection(gt_set))
    total_reward += correct_elements * 0.25  # Reward for correct elements
#   total_reward -= incorrect_elements * 1 # Penalty for incorrect elements

  # Check for the ultimate goal

  if num_perfect_lists == len(ground_truth_lists):
    total_reward += 5 # Goal-level reward

  return total_reward

if __name__ == "__main__":
    model_name = "Qwen/Qwen3-1.7B"

    # load the tokenizer and the model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        # device_map="auto",
    )


    dataset = load_dataset("tm21cy/NYT-Connections", split='train')
    reasoning_start = "<think>" # Acts as <think>
    reasoning_end   = "</think>"   # Acts as </think>
    solution_start  = "<answer>"
    solution_end    = "</answer>"

    system_prompt = \
        f"""You will be playing a categorization game. You will be given a list of 16 words that are all loosely related. However, these words can be grouped into 4 distinct categories of 4 words each, such that the words in each group are more closely related to each other than to the rest. Your task is to identify these 4 groups and return them in the following JSON format: [[\"word1\", \"word2\", \"word3\", \"word4\"], [\"word5\", \"word6\", \"word7\", \"word8\"]].
        Important rules:
        - Each word must appear in exactly one group.
        You will be provided with an example on how to solve it. Provide your answer between {solution_start} and {solution_end}. Don't forget to add quotes on each word and never add anything extra. Think about the problem and provide your reasoning steps between {reasoning_start} and {reasoning_end}."""
    one_shot_prompt = \
    f"""{reasoning_start}The first group is "AWESOME", since they are all words related to awesomeness, although they can also describe something else, it is better to rank them like this. The second group is "VARIETY", the third one is "GIST", since the words are related to getting gist of things, and the last one is "FRIED APPETIZER INFORMALLY", since those words are used to refer to fried foods informally. Understood, now I will output json.{reasoning_end}{solution_start}[["COOL", "NICE", "SICK", "SWEET"], ["KIND", "SORT", "STYLE", "TYPE"], ["DRIFT", "IDEA", "MESSAGE", "POINT"], ["RING", "STICK", "TENDER", "WING"]]{solution_end}"""
    dataset = dataset.map(lambda x: {
        "prompt" : [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "\"KIND\", \"DRIFT\", \"TENDER\", \"NICE\", \"IDEA\", \"WING\", \"SORT\", \"RING\", \"TYPE\", \"STICK\", \"STYLE\", \"SWEET\", \"MESSAGE\", \"SICK\", \"COOL\", \"POINT\""},
            {"role": "assistant", "content": one_shot_prompt},
            {"role": "user",   "content": ", ".join([f'"{word}"' for word in x["words"]])},
        ],
        "answer": [words['words'] for words in x["answers"]],
    })
    split_ratio = 0.1
    split_index = int(len(dataset) * (1 - split_ratio))

    train_dataset = dataset.select(range(split_index))
    eval_dataset = dataset.select(range(split_index, len(dataset)))

    print(train_dataset)
    print(eval_dataset)
        # Add optional EOS token matching
    solution_end_regex = r"</answer>[\s]{0,}" + \
            "(?:" + re.escape(tokenizer.eos_token) + ")?"

    match_format = re.compile(
            rf"{reasoning_end}.*?"\
            rf"{solution_start}(.+?){solution_end_regex}"\
            rf"[\s]{{0,}}$",
            flags = re.MULTILINE | re.DOTALL
    )
    def match_format_exactly(completions, **kwargs):
        scores = []
        for completion in completions:
            score = 0
            response = completion[0]["content"]
            # Match if format is seen exactly!
            if match_format.search(response) is not None: score += 1.0
            scores.append(score)
        return scores
    def match_format_approximately(completions, **kwargs):
        scores = []
        for completion in completions:
            score = 0
            response = completion[0]["content"]
            # Count how many keywords are seen - we penalize if too many!
            # If we see 1, then plus some points!

            score += 0.125 if response.count(reasoning_start) == 1 else 0.0
            score += 0.125 if response.count(reasoning_end)   == 1 else 0.0
            score += 0.125 if response.count(solution_start)  == 1 else 0.0
            score += 0.125 if response.count(solution_end)    == 1 else 0.0
            scores.append(score)
        return scores
    def check_numbers(prompts, completions, answer, **kwargs):
        question = prompts[0][-1]["content"]
        responses = [completion[0]["content"] for completion in completions]
        extracted_responses = []
        for r in responses:
            try:
                extracted_responses.append(ast.literal_eval(match_format.findall(r)[0].strip()))
            except:
                extracted_responses.append(None)
        # extracted_responses = [ast.literal_eval(match_format.findall(r)[0].strip()) for r in responses]
        scores = []
        # Print only every few steps
        global PRINTED_TIMES
        global PRINT_EVERY_STEPS
        if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:
            print(
                '*'*20 + f"Question:\n{question}", f"\nAnswer:\n{answer[0]}", f"\nResponse:\n{responses[0]}", f"\nExtracted:\n{extracted_responses[0]}"
            )
        PRINTED_TIMES += 1

        for guess, true_answer in zip(extracted_responses, answer):

            if guess is None:
                scores.append(0.0)
                continue
            # Convert to numbers
            try:
                score = 0
                # true_answer = float(true_answer.strip())
                # # Remove commas like in 123,456
                # guess       = float(guess.strip().replace(",", ""))
                # scores.append(3.5 if guess == true_answer else -1.5)
                canonical_prediction = canonicalize_groups(guess)
                canonical_truth = canonicalize_groups(true_answer)
                # for list1, list2 in zip(guess, true_answer):
                #     score += (Counter(list1) == Counter(list2))
                scores.append(structured_reward(canonical_prediction, canonical_truth))
            except:
                scores.append(0)
                continue
        return scores




    






    lora_config = LoraConfig(
        task_type="CAUSAL_LM",
        r=32,
        lora_alpha=128,
        target_modules="all-linear",
        lora_dropout = 0.05
    )
    model = get_peft_model(model, lora_config)
    print(model.print_trainable_parameters())


    training_args = GRPOConfig(
        temperature = 0.7,
        warmup_ratio = 0.1,
        weight_decay = 0.1,
        learning_rate = 1e-6,
        lr_scheduler_type = "cosine_with_min_lr",
        lr_scheduler_kwargs = {"min_lr_rate": 0.1},
        bf16 = True,
        use_vllm = True,
        vllm_mode="server",
        log_level = "info",
        logging_first_step = True,
        logging_steps = 1,
        logging_strategy = "steps",
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 4, # Increase to 4 for smoother training
        gradient_checkpointing = True,
        # gradient_checkpointing_kwargs = {"use_reentrant": False},
        max_grad_norm = 0.1,
        num_generations = 4, # Decrease if out of memory
        max_prompt_length = 512,
        max_completion_length = 3072,
        num_train_epochs = 1, # Set to 1 for a full training run
        save_strategy = "steps",
        save_steps = 100,
        output_dir = "outputs",
        overwrite_output_dir = True,
        report_to=["wandb"],
        # For evaluation
        # do_eval = True,
        # per_device_eval_batch_size = 4,
        # eval_accumulation_steps = 1,
        # eval_strategy = "steps",
        # eval_steps = 5,
    )
    # For optional training + evaluation
    # new_dataset = dataset.train_test_split(test_size = 0.01)

    trainer = GRPOTrainer(
        model = model,
        processing_class = tokenizer,
        reward_funcs = [
            match_format_exactly,
            match_format_approximately,
            check_numbers,
        ],
        args = training_args,
        train_dataset = train_dataset,
        eval_dataset = eval_dataset

        # For optional training + evaluation
        # train_dataset = new_dataset["train"],
        # eval_dataset = new_dataset["test"],
    )
    wandb.init(project="GRPO")

    trainer.train()
