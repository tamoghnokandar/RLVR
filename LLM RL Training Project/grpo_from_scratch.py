# -*- coding: utf-8 -*-
"""NanoVLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZfdbD8os4LUzx60lY4MyCI7lihum41oK
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# git clone https://github.com/huggingface/nanoVLM.git
# cd nanoVLM
#

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# uv init --bare --python 3.12
# uv sync --python 3.12
# source .venv/bin/activate
# uv add torch numpy torchvision pillow datasets huggingface_hub transformers wandb



# tokenizer = get_tokenizer(model.cfg.lm_tokenizer, model.cfg.vlm_extra_tokens)
# image_processor = get_image_processor(model.cfg.vit_img_size)

# messages = [{"role": "user", "content": tokenizer.image_token * model.cfg.mp_image_token_length + prompt}]
# encoded_prompt = tokenizer.apply_chat_template([messages], tokenize=True, add_generation_prompt=True)
# tokens = torch.tensor(encoded_prompt).to(device)



# gen = model.generate(tokens, img_t, max_new_tokens=512)
# out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]

# num_generations = 8
# max_prompt_length = 1024
# max_completion_length = 512
# import torch
# completion_ids_batch = torch.tensor([], device=device, dtype=int)
# prompt_ids_batch = torch.tensor([], device=device, dtype=int)
# prompt_mask_batch = torch.tensor([], device=device, dtype=int)
# for i in range(num_generations):
#     prompt_ids, prompt_mask = encoded["input_ids"], encoded["attention_mask"]
#     print(prompt_ids)
#     prompt_ids = prompt_ids[:, -max_prompt_length:]
#     prompt_mask = prompt_mask[:, -max_prompt_length:]
#     prompt_ids = prompt_ids.to(device)
#     prompt_mask = prompt_mask.to(device)
#     prompt_ids_batch = torch.cat((prompt_ids_batch, prompt_ids),dim=0)
#     print(prompt_ids)
#     completions_text = tokenizer.batch_decode(prompt_ids, skip_special_tokens=True)
#     print(completions_text)
#     prompt_completion_ids = model.generate(prompt_ids, img_t, attention_mask=prompt_mask, max_new_tokens=max_completion_length, top_k=50, top_p=0.9, temperature=0.5, greedy=False)
#     print(prompt_completion_ids)
#     print(prompt_completion_ids.shape)
#     completions_text = tokenizer.batch_decode(prompt_completion_ids, skip_special_tokens=True)
#     print(completions_text)

#     completion_ids_batch = torch.cat((completion_ids_batch, prompt_completion_ids),dim=0)


#     completions_text = tokenizer.batch_decode(prompt_completion_ids, skip_special_tokens=True)
#     print(completions_text)

#     print(completion_ids_batch)
#     prompt_mask_batch = torch.cat((prompt_mask_batch, prompt_mask), dim=0)

# Do masking



# !pip install -U datasets

from datasets import load_dataset, Dataset

from models.vision_language_model import VisionLanguageModel
from data.processors import get_tokenizer, get_image_processor
import random
from tqdm import tqdm
import numpy as np
def build_gsm8k_dataloaders(split='train'):
    data = load_dataset('leonardPKU/GEOQA_8K_R1V')[split]

    questions = []
    parsed_answers = []
    images = []
    for i in tqdm(range(len(data)), desc="Processing"):
        # Try to get answer - if is None dont use this sample
        ans = data[i]['answer']
        image = data[i]['images']
        if ans or image is None:
            continue
        else:
            questions.append(data[i]['problem'][7:])
            parsed_answers.append(ans)
            images.append(image)

    # Setup data loaders
    loader = GSM8KLoader(questions, parsed_answers, images)
    # testloader = GSM8KLoader(test_questions.tolist(), test_answers.tolist())

    return loader

from typing import Tuple, Any
from datasets import load_dataset, Dataset
from abc import ABC, abstractmethod
from typing import Tuple, Any

class DataLoader(ABC):
    """
    Abstract base class for data loaders.

    This class defines the interface that all dataset loaders should implement.
    Specific dataset loaders should inherit from this class and implement the
    required methods.

    Attributes:
        random (bool): If True, returns items randomly; if False, returns sequentially
        current_index (int): Current position for sequential access
    """

    def __init__(self, random: bool = False) -> None:
        self.random = random
        self.current_index = 0

    @abstractmethod
    def __len__(self) -> int:
        """Return the total number of items in the dataset."""
        pass

    @abstractmethod
    def __iter__(self) -> 'DataLoader':
        """Return self as iterator."""
        return self

    @abstractmethod
    def __next__(self) -> Any:
        """Return the next item(s) in the dataset."""
        pass


def extract_hash_answer(text: str) -> str | None:
    if "####" not in text:
        return None
    return text.split("####")[1].strip()


class GSM8KLoader(DataLoader):
    """
    A loader class that provides iteration over GSM8K math problems.

    This class implements both sequential and random access to math problems through
    standard Python iterator protocols. It can be used to iterate over problems either
    in order or randomly, making it suitable for both training and evaluation.

    Attributes:
        questions (List[str]): List of math question strings
        answers (List[str]): List of corresponding answer strings
        random (bool): If True, returns problems randomly; if False, returns sequentially
        current_index (int): Current position in the lists for sequential access
    """

    def __init__(self, questions: list[str], answers: list[str], images, random: bool = False) -> None:
        super().__init__(random)
        self.questions = questions
        self.answers = answers
        self.images = images

    def __len__(self) -> int:
        return len(self.questions)

    def __iter__(self) -> 'GSM8KLoader':
        return self

    def __next__(self) -> tuple[str, str]:
        if self.current_index >= len(self.questions):
            raise StopIteration

        if self.random:
            idx = random.randint(0, len(self.questions) - 1)
        else:
            idx = self.current_index
            self.current_index += 1

        return self.questions[idx], self.answers[idx], self.images[idx]

    def reset(self):
        self.current_index = 0

def eval_on_test_set(
    model,
    tokenizer,
    test_loader,
    device: str,
    args,
    round_num: int
) -> tuple[dict[str, float], float]:
    """
    Evaluate model performance on test set.

    Args:
        model: The model to evaluate
        tokenizer: Tokenizer for the model
        test_loader: DataLoader for test set
        device: Device to run on
        args: Training arguments
        round_num: Current training round number

    Returns:
        total_scores: Dictionary of average metrics
        accuracy: Accuracy on test set
    """
    print("Running evaluation on test set...")

    # Track metrics across all test examples
    total_scores = defaultdict(float)
    num_examples = 0
    total_accuracy = 0.0

    # Create log file for this evaluation round
    log_file = os.path.join(args.output_dir, f'eval_metrics_{round_num}.txt')
    test_loader.reset()

    with open(log_file, 'w') as f:
        # Run through test set
        for question, answer, image in tqdm(test_loader, desc="Evaluating on test set"):
            # Generate completions using same function as training
            _, _, _, _, completions_text, _ = generate_completions(
                model, tokenizer, question, image, device, args
            )

            # Score completions using evaluator
            mock_prompts = [[{'content': question}]] * len(completions_text)
            mock_completions = [[{'content': completion}] for completion in completions_text]
            # Make answer array same length as completions
            answers = [answer] * len(completions_text)
            rewards_per_func, metrics = compute_rewards(
                prompts=mock_prompts,
                completions=mock_completions,
                answer=answers,
                device=device
            )

            # Track accuracy and accumulate metrics
            total_accuracy += metrics['accuracy']

            for k, v in metrics.items():
                total_scores[k] += v
            num_examples += 1

            # Log this example
            f.write("\n" + "="*50 + "\n")
            f.write(f"Q# {num_examples}\n")
            f.write(f"Question: {question}\n")
            f.write(f"Response: {completions_text[0]}\n") # Log first completion
            f.write(f"Ground Truth: {answer}\n")
            f.write("Metrics:\n")
            for metric, value in metrics.items():
                f.write(f"{metric}: {value}\n")
            f.write(f"Total Score: {rewards_per_func.sum().item()}\n")


    # Calculate averages
    avg_scores = {k: v/num_examples for k,v in total_scores.items()}
    accuracy = total_accuracy / num_examples * 100

    # Save metrics
    metrics_path = os.path.join(args.output_dir, f'eval_metrics_{round_num}.json')
    with open(metrics_path, 'w') as f:
        json.dump({**avg_scores, 'accuracy': accuracy}, f, indent=4)

    if args.verbose:
        print("\nEvaluation Results:")
        print("-" * 20)
        print(f"Accuracy: {accuracy:.2f}%")
        for metric, value in avg_scores.items():
            print(f"{metric:15s}: {value:.4f}")
        print("-" * 20)

    return avg_scores, accuracy

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def _correctness_reward(prompts, completions, answers):
        """Reward for correct answer."""
        responses = [completion[0]['content'] for completion in completions]
        questions = [prompt[0]['content'] for prompt in prompts]
        model_path = "TIGER-Lab/general-verifier"
        # Load tokenizer and model
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()

        # Example inputs


        # Create prompt
        correctness_rewards = []

        for r,q,a in zip(responses,questions,answers):
            prompt = (
                f"User: ### Question: {q}\n\n"
                f"### Ground Truth Answer: {a}\n\n"
                f"### Student Answer: {r}\n\n"
                "For the above question, please verify if the student's answer is equivalent to the ground truth answer.\n"
                "Do not solve the question by yourself; just check if the student's answer is equivalent to the ground truth answer.\n"
                "If the student's answer is correct, output \"Final Decision: Yes\". If the student's answer is incorrect, output \"Final Decision: No\". Assistant:"
            )

            # Tokenize and generate
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            outputs = model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.0,
                do_sample=False
            )

            output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(type(output_text),output_text)

            if output_text.strip().endswith("Final Decision: Yes"):
                correctness_rewards.append(1.0)
                print("YES")
            else:
                correctness_rewards.append(0.0)
                print("NO")

        return correctness_rewards
def compute_rewards(
        prompts, #: List[List[Dict[str, str]]],
        completions, #: List[List[Dict[str, str]]],
        answer, #: Any,
        device #: str
    ): #-> Tuple[torch.Tensor, Dict[str, float]]:
        """Compute all rewards for the given completions."""

        num_completions = len(completions)
        rewards_per_func = torch.zeros(num_completions, 1, device=device)

        # Compute all reward functions
        all_scores = [
            _correctness_reward(prompts, completions, answer),
            # self._int_format_reward(completions),
            # self._strict_format_reward(completions),
            # self._soft_format_reward(completions),
            # self._xml_count_reward(completions)
        ]

        # Fill rewards tensor
        for i, scores in enumerate(all_scores):
            rewards_per_func[:, i] = torch.tensor(scores, dtype=torch.float32, device=device)
        print("rewards_per_func", rewards_per_func)
        # Compute metrics
        reward_per_func = rewards_per_func.mean(0)

        # Calculate accuracy (perfect correctness score)
        correctness_scores = rewards_per_func[:, 0]  # First reward function is correctness
        num_perfect = (correctness_scores == 1.0).sum().item()
        accuracy = num_perfect / num_completions

        metrics = {
            "rewards/correctness_reward_func": reward_per_func[0].item(),
            # "rewards/int_reward_func": reward_per_func[1].item(),
            # "rewards/strict_format_reward_func": reward_per_func[2].item(),
            # "rewards/soft_format_reward_func": reward_per_func[3].item(),
            # "rewards/xmlcount_reward_func": reward_per_func[4].item(),
            "reward": rewards_per_func.sum(dim=1).mean().item(),
            "accuracy": accuracy
        }

        return rewards_per_func, metrics

def generate_completions(model, tokenizer, question, image, device, args):
    image_processor = get_image_processor(model.cfg.vit_img_size)
    template = f"{tokenizer.image_token * model.cfg.mp_image_token_length}Question: {question} Answer:"
    encoded = tokenizer.batch_encode_plus([template], return_tensors="pt")
    prompt_ids, prompt_mask = encoded["input_ids"], encoded["attention_mask"]
    prompt_ids = prompt_ids[:, -args.max_prompt_length:]
    prompt_mask = prompt_mask[:, -args.max_prompt_length:]
    prompt_ids = prompt_ids.repeat(args.num_generations, 1)
    prompt_mask = prompt_mask.repeat(args.num_generations, 1)
    prompt_ids = prompt_ids.to(device)
    prompt_mask = prompt_mask.to(device)
    img_t = image_processor(image).unsqueeze(0).to(device)
    img_t = img_t.repeat(args.num_generations, 1, 1, 1)
    completion_ids = model.generate(prompt_ids, img_t, attention_mask=prompt_mask, max_new_tokens=args.max_completion_length, top_k=50, top_p=0.9, temperature=0.5, greedy=False)
    completions_text = tokenizer.batch_decode(completion_ids, skip_special_tokens=True)
    is_eos = completion_ids == tokenizer.eos_token_id
    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)
    eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]
    sequence_indices = torch.arange(is_eos.size(1), device=device).expand(is_eos.size(0), -1)
    completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()
    attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
    return prompt_ids, completion_ids, attention_mask, completions_text

def get_reward_breakdown(reward_scores: torch.Tensor):
        """Convert reward scores tensor to labeled dictionary."""
        return {
            'correctness': reward_scores[0].item()
        }
def score_completions(completions_text: list[str], question: str, answer: str, device: str, args):
    log_data = {
        'prompt': {
            'text': question,
            'answer': answer
        },
        'generations': []
    }
    # Format inputs as expected by evaluator
    mock_prompts = [[{'content': question}]] * len(completions_text)
    mock_completions = [[{'content': completion}] for completion in completions_text]
    answers = [answer] * len(completions_text)
    rewards_per_func, metrics = compute_rewards(
        prompts=mock_prompts,
        completions=mock_completions,
        answer=answers,
        device=device
    )
    rewards = rewards_per_func.sum(dim=1)

    # Store generation data
    for i, (completion, reward_scores) in enumerate(zip(completions_text, rewards_per_func)):
        generation_data = {
            'response': completion,
            'scores': {
                **get_reward_breakdown(reward_scores),
                'total_reward': rewards[i].item()
            }
        }
        log_data['generations'].append(generation_data)

    # Compute advantages
    mean_grouped_rewards = rewards.view(-1, num_generations).mean(dim=1)
    std_grouped_rewards = rewards.view(-1, num_generations).std(dim=1)

    mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(num_generations, dim=0)
    std_grouped_rewards = std_grouped_rewards.repeat_interleave(num_generations, dim=0)

    advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)
    metrics["reward_std"] = std_grouped_rewards.mean().item()

    # Store summary statistics
    log_data['summary_stats'] = {
        'mean_rewards_per_group': mean_grouped_rewards.tolist(),
        'std_rewards_per_group': std_grouped_rewards.tolist(),
        'advantages': advantages.tolist()
    }
    return rewards, advantages, rewards_per_func, metrics, log_data

def selective_log_softmax(logits, index):
    """
    A memory-efficient implementation of the common `log_softmax -> gather` operation.

    This function is equivalent to the following naive implementation:
    ```python
    logps = torch.gather(logits.log_softmax(-1), dim=-1, index=index.unsqueeze(-1)).squeeze(-1)
    ```

    Args:
        logits (`torch.Tensor`):
            Logits tensor of shape `(..., num_classes)`.
        index (`torch.Tensor`):
            Index tensor of shape `(...)`, specifying the positions to gather from the log-softmax output.

    Returns:
        `torch.Tensor`:
            Gathered log probabilities with the same shape as `index`.
    """
    if logits.dtype in [torch.float32, torch.float64]:
        selected_logits = torch.gather(logits, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)
        # loop to reduce peak mem consumption
        logsumexp_values = torch.stack([torch.logsumexp(lg, dim=-1) for lg in logits])
        per_token_logps = selected_logits - logsumexp_values  # log_softmax(x_i) = x_i - logsumexp(x)
    else:
        # logsumexp approach is unstable with bfloat16, fall back to slightly less efficent approach
        per_token_logps = []
        for row_logits, row_labels in zip(logits, index):  # loop to reduce peak mem consumption
            row_logps = F.log_softmax(row_logits, dim=-1)
            row_per_token_logps = row_logps.gather(dim=-1, index=row_labels.unsqueeze(-1)).squeeze(-1)
            per_token_logps.append(row_per_token_logps)
        per_token_logps = torch.stack(per_token_logps)
    return per_token_logps

def get_per_token_logps(model, input_ids, img_t, attention_mask, logits_to_keep):
    # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
    # logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
    # logits = logits[:, :-1, :]  # (B, L-1, V), exclude the last logit: it corresponds to the next token pred
    logits, _ = model(input_ids=input_ids, image=img_t, attention_mask=attention_mask)
    print(logits.shape)
    return
    input_ids = input_ids[:, -logits_to_keep:]
    # For transformers<=4.48, logits_to_keep argument isn't supported, so here we drop logits ourselves.
    # See https://github.com/huggingface/trl/issues/2770
    logits = logits[:, -logits_to_keep:]
    return selective_log_softmax(logits, input_ids)  #  compute logprobs for the input tokens

def compute_loss(
    model,
    base_model,
    prompt_ids: torch.Tensor,
    completion_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    completion_mask: torch.Tensor,
    advantages: torch.Tensor,
    img_t
):
    """
    Compute the GRPO loss between current and base model.

    Args:
        model: The current model being trained
        base_model: The reference model to compare against
        prompt_completion_ids: Combined prompt and completion token IDs
        prompt_ids: Token IDs for just the prompt
        completion_ids: Token IDs for just the completion
        attention_mask: Attention mask for the full sequence
        completion_mask: Mask indicating which tokens are from the completion
        advantages: Advantage values for each sequence
        args: Training arguments

    Returns:
        loss: The computed GRPO loss
        metrics: Dictionary containing additional metrics like KL divergence
    """

    # Only need the generated tokens' logits
    logits_to_keep = completion_ids.size(1)
    input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
    # Get reference model logits
    with torch.inference_mode():
        print(logits_to_keep)
        ref_per_token_logps = get_per_token_logps(base_model, input_ids, img_t, attention_mask, logits_to_keep)

    # Get training model logits

    per_token_logps = get_per_token_logps(model, input_ids, img_t, attention_mask, logits_to_keep)

    # Compute KL divergence
    per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1

    # Compute loss with advantages
    per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(1)
    per_token_loss = -(per_token_loss - args.kl_weight_beta * per_token_kl)
    loss = ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()

    # Additional metrics
    metrics = {}
    response_length = completion_mask.sum(1).float().mean().item()
    metrics["response_length"] = response_length
    mean_kl = ((per_token_kl * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
    metrics["kl"] = mean_kl.item()

    return loss, metrics

def grpo_loss(model, base_model, tokenizer, question, answer, image, device, round_num, train_log_dir, args):
    prompt_ids, completion_ids, attention_mask, completions_text = generate_completions(model, tokenizer, question, image, device, args)
    rewards, advantages, rewards_per_func, metrics, log_data = score_completions(completions_text, question, answer, device, args)
    # Write log data
    log_file = os.path.join(training_log_dir, f'{round_num}_generations.txt')
    utils.write_generation_log(log_data, log_file)

    # Compute loss
    completion_mask = attention_mask[:, prompt_ids.size(1):]
    loss, loss_metrics = compute_loss(
        model, base_model, prompt_completion_ids, prompt_ids, completion_ids,
        attention_mask, completion_mask, advantages, args
    )

    # Combine metrics
    metrics.update(loss_metrics)

    return loss, metrics

def parse_args():
    parser = argparse.ArgumentParser(description="GRPO training arguments")

    # Model configuration
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-1.5B-Instruct", help="Name/path of base model")
    parser.add_argument("--dataset_name", type=str, default="gsm8k", help="Dataset to use for training")
    parser.add_argument("--evaluator", type=str, default="gsm8k", help="Evaluator to use for scoring")

    # Output and logging
    parser.add_argument("--output_dir", type=str, default="output", help="Directory to save outputs")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    parser.add_argument("--save_steps", type=int, default=100, help="Save model every N steps")
    parser.add_argument("--eval_iterations", type=int, default=20, help="Number of iterations for evaluation")

    # Optimization hyperparameters
    parser.add_argument("--learning_rate", type=float, default=5e-6, help="Learning rate")
    parser.add_argument("--adam_beta1", type=float, default=0.9, help="Adam beta1")
    parser.add_argument("--adam_beta2", type=float, default=0.99, help="Adam beta2")
    parser.add_argument("--weight_decay", type=float, default=0.1, help="Weight decay")
    parser.add_argument("--max_grad_norm", type=float, default=0.1, help="Max gradient norm for clipping")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4, help="Number of gradient accumulation steps")
    parser.add_argument("--warmup_percent", type=float, default=0.18, help="Percentage of total steps for warmup")
    parser.add_argument("--update_ref_model", action="store_true", help="Whether to update reference model")
    parser.add_argument("--update_ref_model_freq", type=int, default=200, help="How often to update reference model")
    parser.add_argument("--ref_model_mixup_alpha", type=float, default=0.1, help="Alpha parameter for reference model mixup")


    # Generation parameters
    parser.add_argument("--temperature", type=float, default=0.9, help="Sampling temperature")
    parser.add_argument("--num_chains", type=int, default=16, help="Number of parallel generation chains")
    parser.add_argument("--max_prompt_length", type=int, default=256, help="Maximum prompt length")
    parser.add_argument("--max_completion_length", type=int, default=786, help="Maximum completion length")

    # Training parameters
    parser.add_argument("--num_train_iters", type=int, default=1000, help="Number of training iterations")
    parser.add_argument("--kl_weight_beta", type=float, default=0.04, help="KL penalty weight")
    parser.add_argument("--seed", type=int, default=7111994, help="Random seed")

    args = parser.parse_args()
    return args

import argparse
import os
import json

def seed_everything(seed: int) -> None:
    """
    Set random seed for reproducibility across multiple libraries.

    This function sets consistent random seeds for Python's random module,
    NumPy, PyTorch (both CPU and CUDA), and configures CUDNN for deterministic
    operation. This ensures reproducible results across multiple runs.

    Args:
        seed: The random seed to use for all random number generators
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # Additional settings for reproducibility
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

if __name__ == "__main__":

    args = parse_args()

    seed_everything(args.seed)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True
    torch.set_float32_matmul_precision('high')

    ###############################
    ## Main Experiment settings ##
    ###############################

    ## Set which model to train
    # model, tokenizer = llms.get_llm_tokenizer(args.model_name, device)
    model = VisionLanguageModel.from_pretrained("lusxvr/nanoVLM-450M").to(device)

    base_model = VisionLanguageModel.from_pretrained("lusxvr/nanoVLM-450M").to(device)

    ## Set which data set
    train_loader, test_loader = build_gsm8k_dataloaders('train'), build_gsm8k_dataloaders('test')


    ##############################


    # Setup logging
    os.makedirs(args.output_dir, exist_ok=True)
    args_dict = vars(args)
    args_path = os.path.join(args.output_dir, 'args.json')
    with open(args_path, 'w') as f:
        json.dump(args_dict, f, indent=4)
    eval_log_dir = os.path.join(args.output_dir, 'eval_logs')
    os.makedirs(eval_log_dir, exist_ok=True)
    train_log_dir = os.path.join(args.output_dir, 'training_logs')
    os.makedirs(train_log_dir, exist_ok=True)


    # Setup optimizer for trainer agent with GRPO config settings
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.weight_decay,
        eps=1e-8
    )

    # Add linear warmup learning rate scheduler
    warmup_steps = int(args.warmup_percent * args.num_train_iters)
    def get_lr(step):
        if step < warmup_steps:
            return (step / warmup_steps)
        return 1.0
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,lr_lambda=get_lr)


    # Begin training
    accumulated_loss = 0
    optimizer.zero_grad()
    train_metrics_total = {}
    for round_num in tqdm(range(args.num_train_iters), desc="Training Progress"):

        # Evaluate on test set every so often
        if round_num % args.eval_iterations == 0:
            eval_metrics, eval_accuracy = eval_on_test_set(
                model=model,
                tokenizer=tokenizer,
                test_loader=test_loader,
                device=device,
                args=args,
                round_num=round_num
            )

            # Save metrics to eval log dir
            metrics_path = os.path.join(eval_log_dir, f'metrics_{round_num}.json')
            with open(metrics_path, 'w') as f:
                json.dump({
                    'metrics': eval_metrics,
                    'accuracy': eval_accuracy
                }, f, indent=4)

        # Slowly update ref model
        if args.update_ref_model and (round_num+1) % args.update_ref_model_freq == 0:
            with torch.no_grad():
                for param, ref_param in zip(model.parameters(), base_model.parameters()):
                    ref_param.data = args.ref_model_mixup_alpha * param.data + (1 - args.ref_model_mixup_alpha) * ref_param.data

        # Get next question
        question, answer, image = next(train_loader)

        #### question, answer, image ####

        # Do GRPO - generate chains, score, compute advantage, compute loss
        total_loss, train_metrics = grpo_loss(model, base_model, tokenizer, question, answer, image, device, round_num, train_log_dir, args)

        # Gradient accumulation
        total_loss = total_loss # / args.gradient_accumulation_steps
        total_loss.backward()
        accumulated_loss += total_loss.item()
        scheduler.step()

        # Step optimizer
        if (round_num + 1) % args.gradient_accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)
            optimizer.step()
            optimizer.zero_grad()

        # Logs
        train_metrics["learning_rate"] = scheduler.get_last_lr()[0]
        train_metrics["loss"] = total_loss.item() * args.gradient_accumulation_steps
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf')).item()
        train_metrics["grad_norm"] = grad_norm
        train_metrics_total[round_num] = train_metrics
        with open(os.path.join(train_log_dir, "train_logs.json"), "w") as f:
            json.dump(train_metrics_total, f, indent=4)
